{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnVDidrXYyh_"
      },
      "outputs": [],
      "source": [
        "# install lib\n",
        "!pip install diffusers transformers accelerate safetensors pillow torch gradio imageio[ffmpeg] --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import lib\n",
        "import torch, os, uuid, random, imageio\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, EulerAncestralDiscreteScheduler\n",
        "from PIL import Image, ImageEnhance\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "ePyjFbL6Y7x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device Setup\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "print(f\"âš¡ Running on {DEVICE.upper()}\")\n",
        "\n",
        "# Scheduler\n",
        "scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\"\n",
        ")\n",
        "\n",
        "# Pipelines\n",
        "txt2img = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=DTYPE,\n",
        "    use_auth_token=HUGGING_FACE_TOKEN\n",
        ").to(DEVICE)\n",
        "txt2img.enable_attention_slicing()\n",
        "\n",
        "img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=DTYPE,\n",
        "    use_auth_token=HUGGING_FACE_TOKEN\n",
        ").to(DEVICE)\n",
        "img2img.enable_attention_slicing()\n",
        "\n",
        "# ðŸ”¹ Function: Generate Image + Animation\n",
        "def generate_media(user_prompt):\n",
        "    bad_prompt = (\n",
        "        \"lowres, blurry, distorted, deformed, bad anatomy, noisy, ugly, pixelated, \"\n",
        "        \"oversaturated, cropped, broken details\"\n",
        "    )\n",
        "\n",
        "    # Random seed\n",
        "    seed = random.randint(11111, 99999)\n",
        "    generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
        "\n",
        "    # Step 1: Text â†’ Image\n",
        "    with torch.autocast(\"cuda\") if DEVICE == \"cuda\" else torch.no_grad():\n",
        "        base_img = txt2img(\n",
        "            prompt=user_prompt,\n",
        "            negative_prompt=bad_prompt,\n",
        "            guidance_scale=12,\n",
        "            num_inference_steps=50,\n",
        "            height=768, width=768,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "\n",
        "    # Step 2: Animation Frames (progressive refinement)\n",
        "    frames = []\n",
        "    current = base_img\n",
        "    for i in range(12):  # 12 frames\n",
        "        strength = 0.45 + (i * 0.03)  # gradually stronger edits\n",
        "        with torch.autocast(\"cuda\") if DEVICE == \"cuda\" else torch.no_grad():\n",
        "            new_img = img2img(\n",
        "                prompt=user_prompt,\n",
        "                negative_prompt=bad_prompt,\n",
        "                image=current,\n",
        "                strength=min(strength, 0.85),\n",
        "                guidance_scale=13,\n",
        "                num_inference_steps=40\n",
        "            ).images[0]\n",
        "            # Slight enhancements\n",
        "            new_img = ImageEnhance.Contrast(new_img).enhance(1.2)\n",
        "            new_img = ImageEnhance.Brightness(new_img).enhance(1.1)\n",
        "            frames.append(new_img)\n",
        "            current = new_img\n",
        "\n",
        "    # Step 3: Save outputs\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    image_path = f\"results/final_{seed}.png\"\n",
        "    video_path = f\"results/anim_{seed}.mp4\"\n",
        "\n",
        "    # Save final image\n",
        "    frames[-1].save(image_path)\n",
        "\n",
        "    # Save animation\n",
        "    imageio.mimsave(video_path, frames, fps=6)  # 6 fps short animation\n",
        "\n",
        "    return frames[-1], image_path, video_path"
      ],
      "metadata": {
        "id": "OlNIhji5Y8AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## AI Image + Animation Generator\")\n",
        "    gr.Markdown(\"Enter your creative prompt and get both **Image & Animation** outputs!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        user_prompt = gr.Textbox(label=\" Prompt\", lines=2, placeholder=\"e.g., A cyberpunk dragon flying in neon city\")\n",
        "        run_btn = gr.Button(\"Generate Media\")\n",
        "\n",
        "    out_img = gr.Image(label=\"Final Image\", type=\"pil\")\n",
        "    out_img_file = gr.File(label=\"Download Image\")\n",
        "    out_vid_file = gr.File(label=\"Download Animation\")\n",
        "\n",
        "    run_btn.click(fn=generate_media, inputs=user_prompt, outputs=[out_img, out_img_file, out_vid_file])\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "QtYoHSC1Y8NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnRXA1fgY8Z3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}